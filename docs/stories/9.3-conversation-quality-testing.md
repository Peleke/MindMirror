# Story 9.3: Conversation Quality Testing

## Status
Draft

## Story
**As a** QA engineer
**I want** manual test scripts for conversation quality assurance
**so that** we validate the agent feels natural and extracts data accurately

## Acceptance Criteria

1. Test script covers happy path: User answers all questions correctly → profile extracted
2. Test script covers edge cases: Invalid input (strings instead of numbers), Off-topic questions, Mid-conversation exit, Edit previous answers
3. Extraction accuracy: Spot-check 20 test conversations, validate extracted profiles match user inputs (>90% accuracy)
4. Manual test script doc: `docs/testing/onboarding-agent-test-script.md`
5. Automated test: Mock conversation scenarios with predefined inputs, validate extraction

## Tasks / Subtasks

- [ ] Create manual test script doc (AC: 1, 2, 4)
  - [ ] Document happy path test case (full conversation, all questions answered)
  - [ ] Document edge cases: Invalid weight ("potato"), Off-topic ("What's the weather?"), Exit mid-conversation
  - [ ] Include screenshots of expected UI states
  - [ ] Pass/Fail checklist for each test case

- [ ] Create automated conversation tests (AC: 5)
  - [ ] Mock WebSocket client sends predefined user inputs
  - [ ] Validate agent responses follow conversation flow
  - [ ] Validate profile extraction at end
  - [ ] Test scenarios: Happy path, Invalid input, Partial completion

- [ ] Perform spot-check extraction accuracy test (AC: 3)
  - [ ] Run 20 test conversations with different inputs
  - [ ] Compare extracted profiles vs. user inputs
  - [ ] Calculate accuracy rate (correct fields / total fields)
  - [ ] Target: >90% accuracy

- [ ] Document conversation quality metrics
  - [ ] Tone: Casual, friendly, encouraging (not robotic)
  - [ ] Response time: <2 seconds per agent message
  - [ ] Completion time: 1-3 minutes average
  - [ ] User satisfaction: Qualitative feedback from 5 testers

## Dev Notes

### Relevant Source Tree
- **Location:** `docs/testing/onboarding-agent-test-script.md`
- **Automated Tests:** `onboarding-agent/tests/test_conversation_scenarios.py`

### Key Technical Details
- Manual testing for conversation tone (human judgment required)
- Automated testing for functional correctness (extraction, validation)
- Spot-checks for accuracy (manual review of 20 conversations)

### Implementation Notes
```markdown
# Manual Test Script

## Test Case 1: Happy Path
**Steps:**
1. Open app, tap "Get Started" on onboarding launch screen
2. Chat opens, agent asks "What's your name?"
3. Type "Alex", send
4. Agent responds: "Great to meet you, Alex! What would you like to set up today?"
5. Tap "Workouts & Movement" button
6. ... (continue through all questions)
7. Confirmation screen shows extracted data as cards
8. Tap "Looks good!"
9. Completion screen appears with confetti

**Expected Results:**
- All questions asked in sequence
- Agent tone feels friendly, not robotic
- Confirmation screen data matches user inputs
- Profile saved to database (verify via GraphQL query)

**Pass/Fail:** ☐ Pass ☐ Fail

## Test Case 2: Invalid Input
**Steps:**
1. Agent asks "What's your current weight?"
2. Type "potato", send
3. Agent should respond: "I didn't catch that. Could you confirm your weight in pounds or kilograms?"
4. Type "180 lbs", send

**Expected Results:**
- Agent gracefully handles invalid input
- No error shown to user
- Conversation continues normally

**Pass/Fail:** ☐ Pass ☐ Fail
```

```python
# Automated test example
import pytest
from onboarding_agent.conversation_flow import run_conversation

@pytest.mark.asyncio
async def test_happy_path_conversation():
    """Test full conversation with valid inputs"""
    user_inputs = [
        "Alex",  # Name
        "Workouts & Movement",  # Vertical
        "180 lbs",  # Weight
        "5'10\"",  # Height
        "Omnivore",  # Eating preference
        "Strength Training",  # Training preference
        "Build muscle",  # Goal
        "Looks good!"  # Confirmation
    ]

    profile = await run_conversation(user_inputs)

    assert profile.name == "Alex"
    assert profile.eating_profile.weight == 180
    assert profile.eating_profile.weight_unit == "lbs"
    assert "strength_training" in profile.movement_profile.training_preferences
```

### Testing
**Test file location:** `docs/testing/onboarding-agent-test-script.md`

**Test standards:**
- Manual tests for conversation tone and UX
- Automated tests for functional correctness
- Spot-checks for extraction accuracy (20 conversations)

**Testing frameworks:**
- Manual testing (human testers)
- pytest for automated tests
- Logfire for conversation replay and analysis

**Specific testing requirements:**
- Manual test script executable by non-technical QA tester
- Automated tests cover 100% of conversation states
- Spot-check accuracy >90%
- User testing with 5 real users (qualitative feedback)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-20 | v1.0 | Initial story creation from Epic 9 | Mary (Business Analyst) |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent_

### Debug Log References
_To be populated by dev agent_

### Completion Notes
_To be populated by dev agent_

### File List
_To be populated by dev agent_

## QA Results
_To be populated by QA agent_
